{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIME1\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "# itertools — Functions creating iterators for efficient looping\n",
    "# The module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. \n",
    "from itertools import product,combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[itertools](https://docs.python.org/3/library/itertools.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test_data/corpus-q3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persons of verbs\n",
    "# [1-3][SP] = [(eu,nós),(tu,vós),(ele,eles)]\n",
    "\n",
    "nums = ['1','2','3']\n",
    "lts = ['S','P']\n",
    "\n",
    "combinations = product(nums,lts, repeat=1)\n",
    "combinations = product(combinations, repeat=2)\n",
    "\n",
    "# Toal combinations for directionality augmentation\n",
    "mylist = []\n",
    "for i in combinations:\n",
    "    \n",
    "    left = i[0][0]+i[0][1] #left side of the verb\n",
    "    rigth = i[1][0]+i[1][1] #rigth side of the verb\n",
    "    \n",
    "    mylist.append((left,rigth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directionality_augmentation(tuple_string):\n",
    "    '''Directionality augmentation of phrase gr, miss gi'''\n",
    "    \n",
    "    # Lembrar de não usar \\w , substituir por: REGEX_LATIN = 'A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç'\n",
    "    w ='[A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç]+'\n",
    "\n",
    "    pattern1 = '[1-3][SP](_'+w+'_)[1-3][SP]'\n",
    "    pattern2 = '[1-3][SP]_'+w+'_[1-3][SP]'\n",
    "\n",
    "    x = re.findall(pattern1,tuple_string[1])\n",
    "\n",
    "    left_verb = re.findall('(.+)'+pattern2,\n",
    "                           tuple_string[1])#pegando o lado esquerdo do verbo\n",
    "\n",
    "    right_verb = re.findall(pattern2+'(.+)',\n",
    "                            tuple_string[1])#pegando o lado direito do verbo\n",
    "\n",
    "    result_all = []\n",
    "    for person in mylist:\n",
    "        verb = person[0] + x[0] + person[1]\n",
    "\n",
    "        result_all.append(left_verb[0] + verb[0] + right_verb[0])\n",
    "        \n",
    "    return result_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_string = (df['gr'][0],df['gi'][0])\n",
    "\n",
    "#directionality_augmentation(tuple_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TODO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possíveis variáveis de classe\n",
    "agents_dict = {\n",
    "\n",
    "    '1S': ['EU'],\n",
    "    '2S': ['TU'], \n",
    "    '3S': ['ELE', 'ELA'],\n",
    "    '1P': ['NÓS'],\n",
    "    '2P': ['VÓS'],\n",
    "    '3P': ['ELES', 'ELAS']\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "pronouns_dict = {\n",
    "\n",
    "    '1S': ['ME'],\n",
    "    '2S': ['TE'],\n",
    "    '3S': ['LHE'],\n",
    "    '1P': ['NÓS'],\n",
    "    '2P': ['VÓS'],\n",
    "    '3P': ['LHES']\n",
    "\n",
    "}\n",
    "regex_latin ='[A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç]+'\n",
    "\n",
    "# agentes\n",
    "agent_pattern = r'\\b(EU|TU|ELE|ELA|NÓS|VÓS|ELES|ELAS)\\b'\n",
    "\n",
    "# verbos\n",
    "verb_pattern = r'\\b({0}R)\\b'.format(regex_latin)\n",
    "# pronome obliquo atono\n",
    "pronoun_pattern = r'\\b(ME|TE|LHE|VOS|NOS|LHES)\\b'\n",
    "\n",
    "pattern_agent_verb = \"%s %s %s\" %(agent_pattern, verb_pattern, agent_pattern)\n",
    "pattern_pronoun_verb = \"%s %s %s\" %(agent_pattern, pronoun_pattern, verb_pattern)\n",
    "\n",
    "phrases = ['EU ME REMEXER MUITO', 'OI EU ABRAÇAR TU PASSARO TU TRABALHAR EU QUIMO', 'NOSSA EU ABRAÇÉRAAR ELE BALEIA', 'COALA CAPIM EU ABRAÇAR VÓS ANIMAL']\n",
    "\n",
    "behind_ahead = r'(.+){0}(.+)'.format(pattern_agent_verb)\n",
    "search_pattern_agent_verb = re.findall(pattern_agent_verb, phrases[1])\n",
    "search_results = re.finditer(behind_ahead, phrases[1])\n",
    "\n",
    "#print(search_pattern_agent_verb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectar os padrões das frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(phrase):\n",
    "    '''Find pattern in phrase, return 2 list with patterns found'''\n",
    "   \n",
    "    regex_latin ='[A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç]+'\n",
    "    # agentes\n",
    "    agent_pattern = r'\\b(EU|TU|ELE|ELA|NÓS|VÓS|ELES|ELAS)\\b'\n",
    "\n",
    "    # verbos\n",
    "    verb_pattern = r'\\b({0}R)\\b'.format(regex_latin)\n",
    "    # pronome obliquo atono\n",
    "    pronoun_pattern = r'\\b(ME|TE|LHE|VOS|NOS|LHES)\\b'\n",
    "    \n",
    "    \n",
    "    pattern_agent_verb = \"%s %s %s\" %(agent_pattern, verb_pattern, agent_pattern)\n",
    "    pattern_pronoun_verb = \"%s %s %s\" %(agent_pattern, pronoun_pattern, verb_pattern)\n",
    "\n",
    "    # resultado do regex para o padrão [agente -> verbo -> receptor]\n",
    "    search_pattern_agent_verb = re.findall(pattern_agent_verb, phrase)\n",
    "  \n",
    "    # resultado do regex para o padrão [agente -> pronome -> verbo]\n",
    "    search_pattern_pronoun_verb = re.findall(pattern_pronoun_verb, phrase)\n",
    "    \n",
    "    return search_pattern_agent_verb, search_pattern_pronoun_verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remontando o código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_string(pattern):\n",
    "    '''Transformando a padrão em [agente -> verbo -> receptor] em string para pesquisar\n",
    "            a parte de trás e da frente do padrão'''\n",
    "    string = ''\n",
    "    for index, part in enumerate(pattern):\n",
    "        if not index:\n",
    "            string +=part\n",
    "        else:\n",
    "            string += ' '+part\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def new_patterns1(pattern):\n",
    "    '''cria uma novas frases a partir de um pattern [agente -> verbo -> receptor]'''\n",
    "    \n",
    "    phrases = []\n",
    "    \n",
    "    verb = pattern[1]\n",
    "    # combinação com todos as possibilidades do dicionário\n",
    "    combinations = product(agents_dict.keys(), agents_dict.keys(), repeat=1)\n",
    "\n",
    "    for item in combinations:\n",
    "        # criação de frases do tipo [EU, TU ...]\n",
    "        # os for's são necessários nos casos de [ELE, ELA, ELES ELAS]\n",
    "        for index_1, item_1  in enumerate(agents_dict[item[0]]):\n",
    "\n",
    "            for index_2, item_2 in enumerate(agents_dict[item[1]]):\n",
    "\n",
    "                gr = f'{item_1} {verb} {item_2}'\n",
    "                #gi = f'{item[0]}_{verb}_{item[1]}'\n",
    "                phrases.append(gr)\n",
    "  \n",
    "    return phrases\n",
    "\n",
    "def new_patterns2(pattern):\n",
    "    '''cria uma novas frases a partir de um pattern [agente -> pronome -> verbo]'''\n",
    "    phrases = []\n",
    "    \n",
    "    verb = pattern[2]\n",
    "   \n",
    "    # combinação com todos as possibilidades do dicionário pronouns_dict\n",
    "    combinations = product(agents_dict.keys(), pronouns_dict.keys(), repeat=1)\n",
    "\n",
    "    for item in combinations:\n",
    "        # criação de frases do tipo [EU, TU ...]\n",
    "        # os for's são necessários nos casos de [ELE, ELA, ELES ELAS]\n",
    "        for index_1, item_1  in enumerate(agents_dict[item[0]]):\n",
    "\n",
    "            for index_2, item_2 in enumerate(pronouns_dict[item[1]]):\n",
    "\n",
    "                gr = f'{item_1} {item_2} {verb}'\n",
    "                #gi = f'{item[0]}_{verb}_{item[1]}'\n",
    "                phrases.append(gr)\n",
    "                \n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assembly_phrase1(phrase,search_pattern_agent_verb,item):\n",
    "    '''Monta a frase dado uma combinção [agente -> verbo -> receptor]'''\n",
    "    pstr = []\n",
    "    for pattern in search_pattern_agent_verb:\n",
    "        pstr.append(for_string(pattern))\n",
    "    \n",
    "    new_phrase = re.sub(pstr[0],item[0],phrase)\n",
    "    \n",
    "    for i in range(1,len(pstr)):\n",
    "        \n",
    "        new_phrase = re.sub(pstr[i],item[i],new_phrase)\n",
    "    \n",
    "    return new_phrase\n",
    "\n",
    "\n",
    "\n",
    "def assembly_phrase2(phrase,search_pattern_pronoun_verb,item):\n",
    "    '''Monta a frase dado uma combinção  [agente -> pronome -> verbo]'''\n",
    "    pstr = []\n",
    "    for pattern in search_pattern_pronoun_verb:\n",
    "        pstr.append(for_string(pattern))\n",
    "    \n",
    "    new_phrase = re.sub(pstr[0],item[0],phrase)\n",
    "    #print(new_phrase)\n",
    "    \n",
    "    for i in range(1,len(pstr)):\n",
    "        \n",
    "        new_phrase = re.sub(pstr[i],item[i],new_phrase)\n",
    "    \n",
    "    return new_phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = ['25 AGOSTO SAIR COM REVISTA BLITZ COLETÂNEA ALMADA 79 QUE REVELAR PRIMEIRAS MAQUETA ROCK PORTUGUÊS REGISTAR 1979 AINDA TEMA CAÇADA GRAVAR CONVENTO CAPUCHOS 1995 JORGE MORRER CAPTAR VIVO CENTRO CULTURAL BELÉM 2013 [PONTO] ANO QUE PASSAR 30 ANO SOBRE MORTE JOSÉ AFONSO OS UHF LANÇAR 27 OUTUBRO HERANÇA ANDARILHO ÁLBUM TRIBUTO MAIOR REFERÊNCIA MÚSICA NACIONAL [PONTO] ESTE DISCO SER DAR CONTINUIDADE FANTÁSTICO OBRA QUE ELE NOS DEIXAR REVELAR SEMENTE HERDAR REVELAR ANTÓNIO 1000º [PONTO] RIBEIRO PRODUTOR PROJETO ACRECENTOU ESTE SER AQUELES DISCO QUE SE FAZER VEZ VIDA [PONTO] SER 7 VERSÃO PUXAR PARA SOM ORA ELÉCTRICO ORA ACÚSTICO QUE SE JUNTAR 3 TEMA GRUPO FIO CONDUTOR LEGADO HERANÇA MAIS IMPORTANTE CANTAUTOR PORTUGUÊS [PONTO] PRIMEIRA AMOSTRA IR TEMA TRAZER OUTRO AMIGO TAMBÉM REVELAR 24 ABRIL 2017 ESTREIA ANTENA 1 COMEMORAR 43 ANO REVOLUÇÃO ABRIL SEGUIR SINGLE COMBOIO DESCENDENTE COM PARTICIPAÇÃO ARMANDO TEIXEIRA [PONTO]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partes a serem modificadas: [] [('EU', 'TE', 'CONHECER')]\n"
     ]
    }
   ],
   "source": [
    "phrases = ['EU TE CONHECER [PONTO]']#'NÓS CONFIAR ELE [PONTO]'\n",
    "new_patterns = []\n",
    "new_p = []\n",
    "for phrase in phrases:\n",
    "\n",
    "    # [agente -> verbo -> receptor] and [agente -> pronome -> verbo]\n",
    "    search_pattern_agent_verb, search_pattern_pronoun_verb = find_pattern(phrase)\n",
    "    \n",
    "    # Shuffle nos pattern encontradas para para retirada de apenas 2 de forma aleatória,\n",
    "    # porque para cada padrão é gerada 64 frases, logo é 64 ^ NumeroDePatterns\n",
    "    random.shuffle(search_pattern_agent_verb)\n",
    "    random.shuffle(search_pattern_agent_verb)\n",
    "    \n",
    "    search_pattern_agent_verb = search_pattern_agent_verb[:2]\n",
    "    search_pattern_pronoun_verb = search_pattern_pronoun_verb[:2]\n",
    "    \n",
    "    print('Partes a serem modificadas:',search_pattern_agent_verb,search_pattern_pronoun_verb)\n",
    "    \n",
    "    #[agente -> verbo -> receptor] \n",
    "    if search_pattern_agent_verb:\n",
    "        \n",
    "        for patterns_in_phrase in search_pattern_agent_verb:\n",
    "            s = new_patterns1(patterns_in_phrase)\n",
    "            new_patterns.append(s)\n",
    "\n",
    "        #combinação dos pattern das frases geradas\n",
    "        combination_patterns= product(*new_patterns, repeat=1)\n",
    "\n",
    "        for cont, item in enumerate(combination_patterns):\n",
    "            new_p.append(assembly_phrase(phrase,search_pattern_agent_verb,item))\n",
    "    \n",
    "    #[agente -> pronome -> verbo]\n",
    "    if search_pattern_pronoun_verb:\n",
    "        \n",
    "        for patterns_in_phrase in search_pattern_pronoun_verb:\n",
    "            \n",
    "            s = new_patterns2(patterns_in_phrase)\n",
    "            new_patterns.append(s)\n",
    "        #combinação dos pattern das frases geradas\n",
    "        combination_patterns= product(*new_patterns, repeat=1)\n",
    "\n",
    "        for cont, item in enumerate(combination_patterns):\n",
    "            #print(search_pattern_pronoun_verb)\n",
    "            new_p.append(assembly_phrase2(phrase,search_pattern_pronoun_verb,item))\n",
    "\n",
    "            \n",
    "    # Shuffle para retornar 50 frases aleatórias\n",
    "    random.shuffle(new_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ELA LHE CONHECER [PONTO]']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_p[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com dados reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data/gr_gi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns = ['gr','gi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or bytes-like object\n"
     ]
    }
   ],
   "source": [
    "# Pegando as frases que possuem a necessidade de augmentation\n",
    "# [agente -> verbo -> receptor] and [agente -> pronome -> verbo]\n",
    "cont = 0\n",
    "phrases_agent_verb = []\n",
    "phrases_pronoun_verb = []\n",
    "for i,phrase in enumerate(test_data['gr']):\n",
    "    \n",
    "    try:\n",
    "        search_pattern_agent_verb, search_pattern_pronoun_verb = find_pattern(phrase)\n",
    "\n",
    "        if search_pattern_agent_verb :\n",
    "            tpl = (test_data['gr'][i],test_data['gi'][i])\n",
    "            phrases_agent_verb.append(tpl)\n",
    "\n",
    "            cont+=1\n",
    "        if search_pattern_pronoun_verb:\n",
    "            tpl = (test_data['gr'][i],test_data['gi'][i])\n",
    "            phrases_pronoun_verb.append(tpl)\n",
    "            cont +=1\n",
    "        #if cont >= 10:\n",
    "            #break\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total de frases [agente -> verbo -> receptor]\n",
    "len(phrases_agent_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total de frases [agente -> pronome -> verbo]\n",
    "len(phrases_pronoun_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NÓS NOS IMPORTAR [PONTO]', 'NÓS IMPORTAR_IMPORTÂNCIA [PONTO]')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_pronoun_verb[random.randint(0,len(phrases_pronoun_verb))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NÓS CONFIAR ELE [PONTO]', 'NÓS CONFIAR ELA [PONTO]')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_agent_verb[random.randint(0,len(phrases_agent_verb))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
