{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "# itertools — Functions creating iterators for efficient looping\n",
    "# The module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. \n",
    "from itertools import product,combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[itertools](https://docs.python.org/3/library/itertools.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TODO</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns para uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possíveis variáveis de classe\n",
    "agents_dict = {\n",
    "    '1S': ['EU'],\n",
    "    '2S': ['TU'], \n",
    "    '3S': ['ELE', 'ELA'],\n",
    "    '1P': ['NÓS'],\n",
    "    '2P': ['VÓS'],\n",
    "    '3P': ['ELES', 'ELAS']\n",
    "}\n",
    "\n",
    "pronouns_dict = {\n",
    "    '1S': ['ME'],\n",
    "    '2S': ['TE'],\n",
    "    '3S': ['LHE'],\n",
    "    '1P': ['NÓS'],\n",
    "    '2P': ['VÓS'],\n",
    "    '3P': ['LHES']\n",
    "}\n",
    "regex_latin ='[A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç]+'\n",
    "\n",
    "#filtro para GI pattern\n",
    "gi_verb_pattern = '[1-3][SP]_('+regex_latin+')_[1-3][SP]'\n",
    "gi_pattern = '[1-3][SP]_'+regex_latin+'_[1-3][SP]'\n",
    "\n",
    "# agentes\n",
    "agent_pattern = r'\\b(EU|TU|ELE|ELA|NÓS|VÓS|ELES|ELAS)\\b'\n",
    "\n",
    "# verbos\n",
    "verb_pattern = r'\\b({0}R)\\b'.format(regex_latin)\n",
    "# pronome obliquo atono\n",
    "pronoun_pattern = r'\\b(ME|TE|LHE|VOS|NOS|LHES)\\b'\n",
    "\n",
    "pattern_agent_verb = \"%s %s %s\" %(agent_pattern, verb_pattern, agent_pattern)\n",
    "pattern_pronoun_verb = \"%s %s %s\" %(agent_pattern, pronoun_pattern, verb_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectar os padrões das frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(phrase):\n",
    "    '''Find pattern in phrase, return 2 list with patterns found'''\n",
    "\n",
    "    # resultado do regex para o padrão [agente -> verbo -> receptor]\n",
    "    search_pattern_agent_verb = re.findall(pattern_agent_verb, phrase)\n",
    "  \n",
    "    # resultado do regex para o padrão [agente -> pronome -> verbo]\n",
    "    search_pattern_pronoun_verb = re.findall(pattern_pronoun_verb, phrase)\n",
    "    \n",
    "    return search_pattern_agent_verb, search_pattern_pronoun_verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria uma novas pattern  [agente -> verbo -> receptor] e [agente -> pronome -> verbo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_patterns(pattern_gr,verb_gi,**kwargs):\n",
    "    '''cria uma novas frases a partir de um pattern [agente -> verbo -> receptor]\n",
    "    ou [agente -> pronome -> verbo]'''\n",
    "    \n",
    "    patterns_gr = []\n",
    "    patterns_gi = []\n",
    "    t_type = kwargs.get('norm')\n",
    "    \n",
    "    if t_type:\n",
    "        verb = pattern_gr[1]\n",
    "        \n",
    "        # combinação com todos as possibilidades do dicionário\n",
    "        combinations = product(agents_dict.keys(), agents_dict.keys(), repeat=1)\n",
    "\n",
    "        for item in combinations:\n",
    "            # criação de frases do tipo [EU, TU ...]\n",
    "            # os for's são necessários nos casos de [ELE, ELA, ELES ELAS]\n",
    "            for index_1, item_1  in enumerate(agents_dict[item[0]]):\n",
    "\n",
    "                for index_2, item_2 in enumerate(agents_dict[item[1]]):\n",
    "\n",
    "                    gr = f'{item_1} {verb} {item_2}'\n",
    "                    gi = f'{item[0]}_{verb_gi}_{item[1]}'\n",
    "                    patterns_gr.append(gr)\n",
    "                    patterns_gi.append(gi)\n",
    "    else:\n",
    "        verb = pattern_gr[2]\n",
    "        \n",
    "        # combinação com todos as possibilidades do dicionário pronouns_dict\n",
    "        combinations = product(agents_dict.keys(), pronouns_dict.keys(), repeat=1)\n",
    "\n",
    "        for item in combinations:\n",
    "            # criação de frases do tipo [EU, TU ...]\n",
    "            # os for's são necessários nos casos de [ELE, ELA, ELES ELAS]\n",
    "            for index_1, item_1  in enumerate(agents_dict[item[0]]):\n",
    "\n",
    "                for index_2, item_2 in enumerate(pronouns_dict[item[1]]):\n",
    "\n",
    "                    gr = f'{item_1} {item_2} {verb}'\n",
    "                    gi = f'{item[0]}_{verb_gi}_{item[1]}'\n",
    "                    patterns_gr.append(gr)\n",
    "                    patterns_gi.append(gi)\n",
    "    \n",
    "    return patterns_gr,patterns_gi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monta nova frase a partir um padrão já criando anteriormente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma padrão [agente -> verbo -> receptor] para string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_string(*args,**kwargs):\n",
    "    '''Transformando a padrão em [agente -> verbo -> receptor] ou [agente -> pronome -> verbo]\n",
    "        em string para pesquisar a parte de trás e da frente do padrão'''\n",
    "    \n",
    "    if kwargs.get('agent_verb'):\n",
    "        search_pattern = kwargs.get('agent_verb')\n",
    "        \n",
    "    elif kwargs.get('pronoun_verb'):\n",
    "        search_pattern = kwargs.get('pronoun_verb')\n",
    "        \n",
    "    strings = []\n",
    "    for pattern in search_pattern: \n",
    "        line = ''\n",
    "        for index, part in enumerate(pattern):\n",
    "            if not index:\n",
    "                line +=part\n",
    "            else:\n",
    "                line += ' '+part\n",
    "        strings.append(line)\n",
    "        \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monta nova frase a partir um padrão já criando anteriorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembly_phrase(phrase,*args,**kwargs):\n",
    "    '''Monta a frase dado [agente -> verbo -> receptor] ou [agente -> pronome -> verbo]'''\n",
    " \n",
    "    new_phrase_gr = phrase[0]\n",
    "    new_phrase_gi = phrase[1]\n",
    "    \n",
    "    combination_gr = kwargs.get('combination_gr')\n",
    "    combination_gi = kwargs.get('combination_gi')\n",
    "    \n",
    "    if kwargs.get('agent_verb'):\n",
    "        search_pattern = kwargs.get('agent_verb')    \n",
    "        pstr = for_string(agent_verb = search_pattern)# Transforma search_pattern em string\n",
    "    \n",
    "    else:    \n",
    "        search_pattern = kwargs.get('pronoun_verb')\n",
    "        pstr = for_string(pronoun_verb = search_pattern)# Transforma search_pattern em string\n",
    "    \n",
    "    try:\n",
    "        gi_verbs = re.findall(gi_pattern,new_phrase_gi)# Retira verbo do lado GI\n",
    "        for i, part_string in enumerate(pstr):\n",
    "            \n",
    "            new_phrase_gr = re.sub(part_string,combination_gr[i],new_phrase_gr)\n",
    "            new_phrase_gi = re.sub(gi_verbs[i],combination_gi[i],new_phrase_gi)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Erro assembly_phrase regex.\\n',e)\n",
    "    \n",
    "    return (new_phrase_gr,new_phrase_gi)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já é possível criar as frases GR para os dois padrões descritos como: [agente -> verbo -> receptor] e [agente -> pronome -> verbo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augmentation(tupla):\n",
    "    new_patterns_gr = []\n",
    "    new_patterns_gi =[]\n",
    "    new_p = []\n",
    "    \n",
    "    phrase = tupla[0]\n",
    "\n",
    "    # [agente -> verbo -> receptor] and [agente -> pronome -> verbo]\n",
    "    search_pattern_agent_verb, search_pattern_pronoun_verb = find_pattern(phrase)\n",
    "    \n",
    "    gi_verbs = re.findall(gi_verb_pattern,tupla[1])\n",
    "    \n",
    "    # Shuffle nos pattern encontradas para para retirada de apenas 2 de forma aleatória,\n",
    "    # porque para cada padrão é gerada 64 frases, logo é 64 ^ NumeroDePatterns\n",
    "    random.shuffle(search_pattern_agent_verb)\n",
    "    random.shuffle(search_pattern_agent_verb)\n",
    "    \n",
    "    search_pattern_agent_verb = search_pattern_agent_verb[:2]\n",
    "    search_pattern_pronoun_verb = search_pattern_pronoun_verb[:2]\n",
    "    \n",
    "    '''[agente -> verbo -> receptor]''' \n",
    "    if search_pattern_agent_verb:\n",
    "        for i, patterns_in_phrase in enumerate(search_pattern_agent_verb):\n",
    "            gr,gi = new_patterns(patterns_in_phrase,gi_verbs[i],norm=1)\n",
    "            \n",
    "            new_patterns_gr.append(gr)\n",
    "            new_patterns_gi.append(gi)          \n",
    "            \n",
    "    '''[agente -> pronome -> verbo]'''\n",
    "    if search_pattern_pronoun_verb:\n",
    "        for i, patterns_in_phrase in enumerate(search_pattern_pronoun_verb):\n",
    "            gr,gi = new_patterns(patterns_in_phrase,gi_verbs[i],norm=0)\n",
    "            \n",
    "            new_patterns_gr.append(gr)\n",
    "            new_patterns_gi.append(gi)\n",
    "        \n",
    "    #combinação dos pattern das frases geradas\n",
    "    combination_patterns = product(*new_patterns_gr, repeat=1)\n",
    "    combination_patterns_gi = product(*new_patterns_gi, repeat=1)\n",
    "    \n",
    "    for cont, item_gr in enumerate(combination_patterns):\n",
    "        item_gi = next(combination_patterns_gi)\n",
    "\n",
    "        if search_pattern_pronoun_verb:\n",
    "            new_p.append(assembly_phrase(tupla,\n",
    "                                         agent_verb = search_pattern_pronoun_verb,\n",
    "                                         combination_gr = item_gr,\n",
    "                                         combination_gi = item_gi))\n",
    "        if search_pattern_agent_verb:\n",
    "            new_p.append(assembly_phrase(tupla,\n",
    "                                     agent_verb = search_pattern_agent_verb,\n",
    "                                     combination_gr = item_gr,\n",
    "                                     combination_gi = item_gi)) \n",
    "    # Shuffle para retornar 50 frases aleatórias\n",
    "    random.shuffle(new_p)\n",
    "    \n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [('EU TE MOSTRAR CIDADE [PONTO]', '1S_MOSTRAR_2S CIDADE [PONTO]'),\n",
    "           ('ELE PEDIR ELA SE ELA CONHECER [PONTO]', '3S_PERGUNTAR_3S CONHECER [PONTO]')]\n",
    "new_phrases = []\n",
    "for tupla in phrases:\n",
    "    \n",
    "    new_phrases.append(augmentation(tupla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU VÓS MOSTRAR CIDADE [PONTO]', '1S_MOSTRAR_2P CIDADE [PONTO]'),\n",
       " ('TU TE MOSTRAR CIDADE [PONTO]', '2S_MOSTRAR_2S CIDADE [PONTO]'),\n",
       " ('ELES LHES MOSTRAR CIDADE [PONTO]', '3P_MOSTRAR_3P CIDADE [PONTO]'),\n",
       " ('VÓS TE MOSTRAR CIDADE [PONTO]', '2P_MOSTRAR_2S CIDADE [PONTO]'),\n",
       " ('ELAS NÓS MOSTRAR CIDADE [PONTO]', '3P_MOSTRAR_1P CIDADE [PONTO]'),\n",
       " ('ELAS TE MOSTRAR CIDADE [PONTO]', '3P_MOSTRAR_2S CIDADE [PONTO]'),\n",
       " ('ELE LHES MOSTRAR CIDADE [PONTO]', '3S_MOSTRAR_3P CIDADE [PONTO]'),\n",
       " ('EU NÓS MOSTRAR CIDADE [PONTO]', '1S_MOSTRAR_1P CIDADE [PONTO]'),\n",
       " ('TU VÓS MOSTRAR CIDADE [PONTO]', '2S_MOSTRAR_2P CIDADE [PONTO]'),\n",
       " ('TU LHES MOSTRAR CIDADE [PONTO]', '2S_MOSTRAR_3P CIDADE [PONTO]'),\n",
       " ('VÓS NÓS MOSTRAR CIDADE [PONTO]', '2P_MOSTRAR_1P CIDADE [PONTO]'),\n",
       " ('NÓS LHES MOSTRAR CIDADE [PONTO]', '1P_MOSTRAR_3P CIDADE [PONTO]'),\n",
       " ('VÓS LHES MOSTRAR CIDADE [PONTO]', '2P_MOSTRAR_3P CIDADE [PONTO]'),\n",
       " ('NÓS LHE MOSTRAR CIDADE [PONTO]', '1P_MOSTRAR_3S CIDADE [PONTO]'),\n",
       " ('TU NÓS MOSTRAR CIDADE [PONTO]', '2S_MOSTRAR_1P CIDADE [PONTO]')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_phrases[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_phrases[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com dados reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data/gr_gi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns = ['gr','gi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or bytes-like object\n"
     ]
    }
   ],
   "source": [
    "# Pegando as frases que possuem a necessidade de augmentation\n",
    "# [agente -> verbo -> receptor] and [agente -> pronome -> verbo]\n",
    "cont = 0\n",
    "phrases_agent_verb = []\n",
    "phrases_pronoun_verb = []\n",
    "for i,phrase in enumerate(test_data['gr']):\n",
    "    \n",
    "    try:\n",
    "        search_pattern_agent_verb, search_pattern_pronoun_verb = find_pattern(phrase)\n",
    "\n",
    "        if search_pattern_agent_verb :\n",
    "            tpl = (test_data['gr'][i],test_data['gi'][i])\n",
    "            phrases_agent_verb.append(tpl)\n",
    "\n",
    "            cont+=1\n",
    "        if search_pattern_pronoun_verb:\n",
    "            tpl = (test_data['gr'][i],test_data['gi'][i])\n",
    "            phrases_pronoun_verb.append(tpl)\n",
    "            cont +=1\n",
    "        #if cont >= 10:\n",
    "            #break\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total de frases [agente -> verbo -> receptor]\n",
    "len(phrases_agent_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total de frases [agente -> pronome -> verbo]\n",
    "len(phrases_pronoun_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EU ME CULPAR [PONTO]', 'EU CULPADO [PONTO]')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_pronoun_verb[random.randint(0,len(phrases_pronoun_verb))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ELES RIR NÓS [PONTO]', 'ELES RIR NÓS [PONTO]')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_agent_verb[random.randint(0,len(phrases_agent_verb))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bbc'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('a','b','abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
