{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TODO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "# itertools — Functions creating iterators for efficient looping\n",
    "# The module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. \n",
    "from itertools import product,combinations\n",
    "\n",
    "class Directionality_Augmentation():\n",
    "    \"\"\"[Directionality augmentation of a phrase for the following pattern \n",
    "    [agent -> verb -> receiver] and [agent -> pronoun -> verb]]\n",
    "    \"\"\"    \n",
    "    \n",
    "    # The name of this pipeline element.\n",
    "    name = 'directionality'\n",
    "    \n",
    "    _agents_dict = {\n",
    "        '1S': ['EU'],\n",
    "        '2S': ['TU'], \n",
    "        '3S': ['ELE', 'ELA'],\n",
    "        '1P': ['NÓS'],\n",
    "        '2P': ['VÓS'],\n",
    "        '3P': ['ELES', 'ELAS']\n",
    "    }\n",
    "    _pronouns_dict = {\n",
    "        '1S': ['ME'],\n",
    "        '2S': ['TE','LHE'],\n",
    "        '1P': ['NOS'],\n",
    "        '2P': ['VOS'],\n",
    "        '3P': ['LHES']\n",
    "    }\n",
    "    _valid_chars ='A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç_'\n",
    "    _high_valid_chars = '['+_valid_chars+']+'\n",
    "    \n",
    "    #GI pattern filter \n",
    "    _gi_verb_pattern = '[1-3][SP]_('+_high_valid_chars+')_[1-3][SP]'\n",
    "    _gi_pattern = '[1-3][SP]_'+_high_valid_chars+'_[1-3][SP]'\n",
    "\n",
    "    # pronome reto\n",
    "    _agent_pattern = r'\\b(EU|TU|ELE|ELA|NÓS|VÓS|ELES|ELAS)\\b'\n",
    "\n",
    "    # verbs\n",
    "    _verb_pattern = r'\\b({0}R|DO_QUE)\\b'.format(_high_valid_chars)\n",
    "    \n",
    "    # pronome obliquo atono\n",
    "    _pronoun_pattern = r'\\b(ME|TE|LHE|VOS|NOS|LHES)\\b'\n",
    "\n",
    "    # both patterns -> [pronome reto, verbo, pronome reto] \n",
    "    # e [pronome reto, pronome obliquo atono, verbo]\n",
    "    _pattern_agent_verb = \"%s %s %s\" %(_agent_pattern, _verb_pattern, _agent_pattern)\n",
    "    _pattern_pronoun_verb = \"%s %s %s\" %(_agent_pattern, _pronoun_pattern, _verb_pattern)\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "\t\t#super.__init__(*args, **kwargs)\n",
    "        pass\n",
    "     \n",
    "    #! Métodos necessários para o step de augmentation\n",
    "    def find_pattern(self,phrase_gr_gi):\n",
    "        \"\"\"[Find pattern in phrase, return 2 lists with each pattern]\n",
    "        \n",
    "        Arguments:\n",
    "            phrase_gr {string} -- [gr type phrase]\n",
    "        \n",
    "        Returns:\n",
    "            [type] -- [2 lists with patterns found or 2 empty lists if \n",
    "                       no pattern is encountered]\n",
    "        \"\"\" \n",
    "        gr = phrase_gr_gi[0]\n",
    "        gi = phrase_gr_gi[1]\n",
    "        search_pattern_agent_verb, search_pattern_gi = None,None\n",
    "        try:     \n",
    "            # regex result for [pronome reto, verbo, pronome reto]\n",
    "            # regex result for [pronome reto, pronome obliquo atono, verbo]\n",
    "            search_pattern_agent_verb = re.findall(self._pattern_agent_verb+'|'+self._pattern_pronoun_verb, gr)\n",
    "            search_pattern_gi = re.findall(self._gi_pattern,gi)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print('find_pattern:',e)\n",
    "            print(phrase_gr_gi)\n",
    "        finally:\n",
    "            return search_pattern_agent_verb, search_pattern_gi\n",
    "            \n",
    "    \n",
    "    def new_patterns(self, pattern_gr, verb_gi, **kwargs):\n",
    "        '''creates new phrases from the following patterns \n",
    "       [pronome reto, verbo, pronome reto] ou [pronome reto, pronome obliquo atono, verbo]'''\n",
    "        patterns_gr = []\n",
    "        patterns_gi = []\n",
    "        \n",
    "        # pattern_gr[-1]  == '' -> [pronome reto, verbo, pronome reto] \n",
    "        # or pattern_gr[0] == '' -> [pronome reto, pronome obliquo atono, verbo]\n",
    "               \n",
    "        # [pronome reto, verbo, pronome reto] -> 1\n",
    "        if pattern_gr[-1] == '':\n",
    "            \n",
    "            verb = pattern_gr[1]\n",
    "            \n",
    "            # Combinations involving all possible dictionary keys of agensts_dict\n",
    "            combinations = product(self._agents_dict.keys(), self._agents_dict.keys(), repeat=1)\n",
    "\n",
    "\n",
    "            for combination in combinations:\n",
    "                # creating phrases of type: [EU, TU ...]\n",
    "                # The other loops are necessary in these following cases: [ELE, ELA, ELES ELAS]\n",
    "                for index_1, directional_1  in enumerate(self._agents_dict[combination[0]]):\n",
    "                    \n",
    "                    for index_2, directional_2 in enumerate(self._agents_dict[combination[1]]):\n",
    "                        \n",
    "                        gr = f'{directional_1} {verb} {directional_2}'\n",
    "                        gi = f'{combination[0]}_{verb_gi}_{combination[1]}'\n",
    "                        patterns_gr.append(gr)\n",
    "                        patterns_gi.append(gi)\n",
    "        \n",
    "        # [pronome reto, pronome obliquo atono, verbo] -> 2\n",
    "        elif pattern_gr[0] == '':\n",
    "            \n",
    "            verb = pattern_gr[-1]\n",
    "            # Combinations involving all possible dictionary keys of pronouns_dict\n",
    "            combinations = product(self._agents_dict.keys(), self._pronouns_dict.keys(), repeat=1)\n",
    "            \n",
    "            for combination in combinations:\n",
    "                # creating phrases of type: [EU, TU ...]\n",
    "                # The other loops are necessary in these following cases: [ELE, ELA, ELES ELAS]\n",
    "                for index_1, directional_1  in enumerate(self._agents_dict[combination[0]]):\n",
    "                    for index_2, directional_2 in enumerate(self._pronouns_dict[combination[1]]):\n",
    "\n",
    "                        gr = f'{directional_1} {directional_2} {verb}'\n",
    "                        gi = f'{combination[0]}_{verb_gi}_{combination[1]}'\n",
    "                        patterns_gr.append(gr)\n",
    "                        patterns_gi.append(gi)\n",
    "        \n",
    "        return patterns_gr,patterns_gi\n",
    "    \n",
    "    \n",
    "    def for_string(*args,**kwargs):\n",
    "        '''Transforms a pattern ([pronome reto -> verbo -> pronome reto] \n",
    "            or [pronome reto -> pronome obliquo -> verbo])\n",
    "            in a string to search for what comes before and after '''\n",
    "        \n",
    "        if kwargs.get('search_pattern'):\n",
    "            search_pattern = kwargs.get('search_pattern')\n",
    "            \n",
    "        list_of_str = []\n",
    "        \n",
    "        # Transforms the pattern in a string to check\n",
    "        # for what comes before and after it in the phrase\n",
    "        \n",
    "        for tuple_pattern  in search_pattern:\n",
    "            pattern_str = ''\n",
    "\n",
    "            for index, str_pattern in enumerate(tuple_pattern):\n",
    "\n",
    "                if str_pattern != '':\n",
    "                    pattern_str += str_pattern + ' '\n",
    "\n",
    "            list_of_str.append(pattern_str[:-1])\n",
    "                                \n",
    "        return list_of_str\n",
    "    \n",
    "    def assembly_phrase(self,phrase,*args,**kwargs):\n",
    "        '''Creates new phrases with => before_part + pattern + after_part \n",
    "        for each combination in new_patterns method [pronome reto -> verbo -> pronome reto] \n",
    "        or [pronome reto -> pronome obliquo -> verb]'''\n",
    " \n",
    "        new_phrase_gr = phrase[0]\n",
    "        new_phrase_gi = phrase[1]\n",
    "        combination_gr = kwargs.get('combination_gr')\n",
    "        combination_gi = kwargs.get('combination_gi')\n",
    "        \n",
    "        if kwargs.get('search_pattern'): \n",
    "            search_pattern = kwargs.get('search_pattern')  \n",
    "            # Transforms search_pattern in a string  \n",
    "            search_pattern_to_string = self.for_string(search_pattern = search_pattern)\n",
    "            \n",
    "        try:\n",
    "            # Gets all patterns in GI\n",
    "            gi_verbs = re.findall(self._gi_pattern, new_phrase_gi)\n",
    "            \n",
    "            # Creates new phrases with new directionalities elements for both gr and gi\n",
    "            for i, part_string in enumerate(search_pattern_to_string):\n",
    "                \n",
    "                new_phrase_gr = re.sub(part_string,combination_gr[i], new_phrase_gr)\n",
    "                new_phrase_gi = re.sub(gi_verbs[i], combination_gi[i], new_phrase_gi)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error assembly_phrase regex.\\n',e)\n",
    "            \n",
    "        return (new_phrase_gr,new_phrase_gi)\n",
    "    \n",
    "    def augmentation(self, gr_gi_tuple, max_new_sentences=50):\n",
    "        \"\"\"[Does the augmentation for both gr, gi]\n",
    "        \n",
    "        Arguments:\n",
    "            gr_gi_tuple {(string, string)} -- [tuple containing (gr_phrase, gi_phrase)]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            max_new_sentences {int} -- [max_new_sentencess the amount of new phrases generated for\n",
    "                            each (gr, gi)] (default: {50})\n",
    "        \n",
    "        Returns:\n",
    "            [list] -- [New phrases generated]\n",
    "        \"\"\"        \n",
    "        new_patterns_gr = []\n",
    "        new_patterns_gi =[]\n",
    "        new_phrases = []\n",
    "        phrase_gr = gr_gi_tuple[0]\n",
    "\n",
    "        # gets both [pronome reto -> verbo -> pronome reto]  \n",
    "        # and [pronome reto -> pronome obliquo -> verbo]\n",
    "        search_pattern_agent_verb , search_pattern_gi = self.find_pattern(gr_gi_tuple)   \n",
    "        \n",
    "        #* [pronome reto -> verbo -> pronome reto]\n",
    "        # [pronome reto -> pronome obliquo -> verbo]\n",
    "        if search_pattern_agent_verb and search_pattern_gi:\n",
    "            \n",
    "            # find all verbs in GI \n",
    "            gi_verbs = re.findall(self._gi_verb_pattern, gr_gi_tuple[1])\n",
    "            # Creates new phrases for each pattern\n",
    "            # [pronome reto -> pronome obliquo -> verbo]\n",
    "            for i, patterns_in_phrase in enumerate(search_pattern_agent_verb):\n",
    "                gr,gi = self.new_patterns(patterns_in_phrase, \n",
    "                                          gi_verbs[i])\n",
    "                new_patterns_gr.append(gr)\n",
    "                new_patterns_gi.append(gi)            \n",
    "\n",
    "\n",
    "            # Combinating patterns of the newly created phrases\n",
    "            combination_patterns_gr = product(*new_patterns_gr)\n",
    "            combination_patterns_gi = product(*new_patterns_gi)\n",
    "\n",
    "            for combination_gr in combination_patterns_gr:\n",
    "                # New Patterns for GI \n",
    "                combination_gi = next(combination_patterns_gi)\n",
    "\n",
    "                # Generates new phrases [befores_string + pattern + after_string] \n",
    "                # if the following pattern is encountered \n",
    "                # [pronome reto -> pronome obliquo -> verbo]\n",
    "                # Generates new phrases [befores_string + pattern + after_string] \n",
    "                # if the following pattern is encountered \n",
    "                # [pronome reto -> verbo -> pronome reto]\n",
    "                new_phrases.append(self.assembly_phrase(gr_gi_tuple,\n",
    "                                                        search_pattern = search_pattern_agent_verb,\n",
    "                                                        combination_gr = combination_gr,\n",
    "                                                        combination_gi = combination_gi))\n",
    "            try:\n",
    "                new_phrases.remove(gr_gi_tuple)\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "            finally:\n",
    "                # Shuffle to return {max_new_sentences ( default_val = 50 )} random phrases\n",
    "                random.shuffle(new_phrases)\n",
    "                new_phrases = new_phrases[:max_new_sentences]\n",
    "                new_phrases.insert(0,gr_gi_tuple)\n",
    "                return new_phrases\n",
    "        else:\n",
    "            new_phrases.insert(0,gr_gi_tuple)\n",
    "            return new_phrases\n",
    "                    \n",
    "               \n",
    "    \n",
    "    def process(self,data, max_new_sentences = 50):\n",
    "        # Used to do the augmentation in phrases\n",
    "        new_phrases = []\n",
    "        for phrase in data:\n",
    "            new_phrases.extend(self.augmentation(phrase, max_new_sentences = max_new_sentences))\n",
    "        return new_phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já é possível criar as frases GR para os dois padrões descritos como: [agente -> verbo -> receptor] e [agente -> pronome -> verbo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com dados reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "_valid_chars ='A-ZÁÉÍÓÚÀÂÊÔÃÕÜÇa-záéíóúàâêôãõüç_'\n",
    "_high_valid_chars = '['+_valid_chars+']+'\n",
    "\n",
    "#GI pattern filter \n",
    "_gi_verb_pattern = '[1-3][SP]_('+_high_valid_chars+')_[1-3][SP]'\n",
    "_gi_pattern = '[1-3][SP]_'+_high_valid_chars+'_[1-3][SP]'\n",
    "\n",
    "# pronome reto\n",
    "_agent_pattern = r'\\b(EU|TU|ELE|ELA|NÓS|VÓS|ELES|ELAS)\\b'\n",
    "\n",
    "# verbs\n",
    "_verb_pattern = r'\\b({0}R|DO_QUE)\\b'.format(_high_valid_chars)\n",
    "\n",
    "# pronome obliquo atono\n",
    "_pronoun_pattern = r'\\b(ME|TE|LHE|VOS|NOS|LHES)\\b'\n",
    "\n",
    "# both patterns -> [pronome reto, verbo, pronome reto] \n",
    "# e [pronome reto, pronome obliquo atono, verbo]\n",
    "_pattern_agent_verb = \"%s %s %s\" %(_agent_pattern, _verb_pattern, _agent_pattern)\n",
    "_pattern_pronoun_verb = \"%s %s %s\" %(_agent_pattern, _pronoun_pattern, _verb_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados equipe traducão vlibras 2019\n",
    " ``datasets/CorpusCondensadoGeral/v1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data/gr_gi.csv')\n",
    "test_data.columns = ['gr','gi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find_pattern: expected string or bytes-like object\n",
      "(nan, 'DESTACAR TAMBÉM PLANURA MONTE ARTIFICIAL NÃO ABAULAR PERFEITO NIVELADO LEMBRAR TERRENO PÁTEO  AMPLO SEM TERRA COBRIR_LOCAL [PONTO]')\n"
     ]
    }
   ],
   "source": [
    "# Pegando as frases que possuem a necessidade de augmentation\n",
    "# [agente -> verbo -> receptor] and [agente -> pronome -> verbo]\n",
    "cont = 0\n",
    "phrases_augmentation = []\n",
    "for i in range (len(test_data)):\n",
    "    try:\n",
    "        phrase = (test_data['gr'][i],test_data['gi'][i])\n",
    "        \n",
    "        search_pattern_gr, search_pattern_gi = da.find_pattern(phrase)\n",
    "        \n",
    "        if search_pattern_gr and search_pattern_gi:\n",
    "            phrases_augmentation.append(phrase)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phrases_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU TE CONHECER [PONTO]', '1S_CONHECER_2S [PONTO]'),\n",
       " ('EU LHE DAR BONECA [PONTO]', '1S_DAR_2S BONECA [PONTO]'),\n",
       " ('EU TE DAR CADERNO [PONTO]', '1S_DAR_2S CADERNO [PONTO]'),\n",
       " ('EU LHE DAR CADERNO [PONTO]', '1S_DAR_2S CADERNO [PONTO]'),\n",
       " ('EU LHE DAR CANETA [PONTO]', '1S_DAR_2S CANETA [PONTO]'),\n",
       " ('EU LHE DAR LIVRO [PONTO]', '1S_DAR_2S LIVRO [PONTO]'),\n",
       " ('EU TE MOSTRAR CIDADE [PONTO]', '1S_MOSTRAR_2S CIDADE [PONTO]'),\n",
       " ('EU LHE PEDIR QUE RECONSIDERAR [PONTO]', '1S_PEDIR_2S RECONSIDERAR [PONTO]'),\n",
       " ('EU PEDIR ELES PARA CONSERTAR MEU CARRO [PONTO]',\n",
       "  '1S_PEDIR_3S CONSERTAR MEU CARRO [PONTO]'),\n",
       " ('ELA LHE PERGUNTAR ONDE ELE MORAR [PONTO]',\n",
       "  '1S_PERGUNTAR_2S MORAR  [PONTO]'),\n",
       " ('EU ME PERGUNTAR PORQUÊ ÔNIBUS ESTAR ATRASADO [PONTO]',\n",
       "  '1S_PERGUNTAR_2S PORQUE ÔNIBUS ATRASAR [PONTO]'),\n",
       " ('EU TE SEGUIR [PONTO]', '1S_SEGUIR_2S [PONTO]'),\n",
       " ('EU TE VER MEUS SONHO [PONTO]', '1S_VER_2S MEU SONHO [PONTO]'),\n",
       " ('EU VER ELA OUTRO DIA [PONTO]', '1S_VER_3S DIA ANTES_PASSADO [PONTO]'),\n",
       " ('EU VER ELA VARRER QUARTO [PONTO]', '1S_VER_3S VARRER QUARTO [PONTO]'),\n",
       " ('25 AGOSTO SAIR COM REVISTA BLITZ COLETÂNEA ALMADA 79 QUE REVELAR PRIMEIRAS MAQUETA ROCK PORTUGUÊS REGISTAR 1979 AINDA TEMA CAÇADA GRAVAR CONVENTO CAPUCHOS 1995 JORGE MORRER CAPTAR VIVO CENTRO CULTURAL BELÉM 2013 [PONTO] ANO QUE PASSAR 30 ANO SOBRE MORTE JOSÉ AFONSO OS UHF LANÇAR 27 OUTUBRO HERANÇA ANDARILHO ÁLBUM TRIBUTO MAIOR REFERÊNCIA MÚSICA NACIONAL [PONTO] ESTE DISCO SER DAR CONTINUIDADE FANTÁSTICO OBRA QUE ELE NOS DEIXAR REVELAR SEMENTE HERDAR REVELAR ANTÓNIO 1000º [PONTO] RIBEIRO PRODUTOR PROJETO ACRECENTOU ESTE SER AQUELES DISCO QUE SE FAZER VEZ VIDA [PONTO] SER 7 VERSÃO PUXAR PARA SOM ORA ELÉCTRICO ORA ACÚSTICO QUE SE JUNTAR 3 TEMA GRUPO FIO CONDUTOR LEGADO HERANÇA MAIS IMPORTANTE CANTAUTOR PORTUGUÊS [PONTO] PRIMEIRA AMOSTRA IR TEMA TRAZER OUTRO AMIGO TAMBÉM REVELAR 24 ABRIL 2017 ESTREIA ANTENA 1 COMEMORAR 43 ANO REVOLUÇÃO ABRIL SEGUIR SINGLE COMBOIO DESCENDENTE COM PARTICIPAÇÃO ARMANDO TEIXEIRA [PONTO]',\n",
       "  '25 AGOSTO SAIR REVISTA BLITZ COLET NEA ALMADA 79 REVELAR PRIMEIRO_ORDINAL MAQUETA ROCK PORTUGAL_PAÍS REGISTRAR ANO_DATA 1979 AINDA_MOMENTO TEMA CAÇAR GRAVAR CONVENTO CAPUCHO ANO_DATA 1995 JORGE MORRER CAPTAR_CONSEGUIR AO_VIVO CENTRO_LOCAL CULTURAL BELÉM_CIDADE 2013 [PONTO] ANO_DATA PASSAR_TEMPO 30 ANO_DATA SOBRE_ASSUNTO MORTE JOSÉ AFONSO UHF LANÇAR_PUBLICAR 27 OUTUBRO HERANÇA ANDARILHO ÁLBUM_MÚSICA TRIBUTO MAIOR REFERÊNCIA MÚSICA DISCO 1S_DAR_2S CONTINUAR FANTÁSTICO OBRA_MÚSICA ELE DEIXAR REVELAR SEMENTE HERDAR REVELAR ANTÔNIO RIBEIRO PRODUTOR PROJETO ACRECENTAR DAQUELE DISCO FAZER 1 VEZ VIDA [PONTO] 7 VERSÃO PUXAR SOM ELÉTRICO ACÚSTICO JUNTAR_UNIR 3 TEMA GRUPO FIO CONDUTOR LEGADO HERANÇA MAIS_QUANTIDADE IMPORTANTE CANTAUTOR PORTUGAL_PAÍS [PONTO] PRIMEIRO_ORDINAL AMOSTRA TEMA TRAZER OUTRO AMIGO TAMBÉM REVELAR 24 ABRIL ANO_DATA 2017 ESTREIAR ANTENA_TV 1 COMEMORAÇÃO 43 ANO_DATA REVOLUÇÃO ABRIL SEGUIR SINGLE COMBOIO DESCENDENTE PARTICIPAR ARMANDO TEIXEIRA [PONTO]'),\n",
       " ('ELA MOSTRAR ELE SEU ASSENTO [PONTO]', '2S_MOSTRAR_3S SEU ASSENTO [PONTO]'),\n",
       " ('ELES ME CHAMAR [PONTO]', '3P_CHAMAR_1S [PONTO]'),\n",
       " ('ELAS ME CHAMAR [PONTO]', '3P_CHAMAR_1S [PONTO]'),\n",
       " ('ELES TE ENCONTRAR [PONTO]', '3P_ENCONTRAR_2S [PONTO]'),\n",
       " ('ELES ME LIGAR [PONTO]', '3P_LIGAR_1S [PONTO]'),\n",
       " ('ELAS ME LIGAR [PONTO]', '3P_LIGAR_1S [PONTO]'),\n",
       " ('ELES LHE PERGUNTAR [PONTO]', '3P_PERGUNTAR_2S [PONTO]'),\n",
       " ('ELE ME FALAR PARA FALAR SIM [PONTO]', '3S_AVISAR_1S DIZER SIM [PONTO]'),\n",
       " ('ELE NOS CONHECER MUITO BEM [PONTO]', '3S_CONHECER_1P BEM_MUITO [PONTO]'),\n",
       " ('ELA ME DAR ESTE 400º [PONTO]', '3S_DAR_1S CD [PONTO]'),\n",
       " ('ELA ME DAR ESTE DISCO COMPACTO [PONTO]',\n",
       "  '3S_DAR_1S DISCO COMPACTO [PONTO]'),\n",
       " ('ELE ME DAR ISSO LIVRE ESPONTÂNEO VONTADE [PONTO]',\n",
       "  '3S_DAR_1S VONTADE LIVRE [PONTO]'),\n",
       " ('ELE ME DAR ISSO ESPONTANEAMENTE [PONTO]',\n",
       "  '3S_DAR_1S VONTADE LIVRE [PONTO]'),\n",
       " ('ELA ME DAR MUITO COMIDA [PONTO]', '3S_DAR_2S (+)COMIDA [PONTO]'),\n",
       " ('ELA LHE DAR PEDAÇO PAPEL [PONTO]', '3S_DAR_2S PEDAÇO PAPEL [PONTO]'),\n",
       " ('ELA ME DAR VÁRIOS LIVRO [PONTO]', '3S_DAR_2S VÁRIOS LIVRO [PONTO]'),\n",
       " ('ELES NOS ENGANAR [PONTO]', '3S_ENGANAR_1P [PONTO]'),\n",
       " ('ELE ME EVITAR [PONTO]', '3S_EVITAR_1S [PONTO]'),\n",
       " ('ELE PEDIR ELA SE ELA CONHECER [PONTO]',\n",
       "  '3S_PERGUNTAR_3S  CONHECER [PONTO]'),\n",
       " ('ELE PERGUNTAR ELA ONDE MORAR [PONTO]',\n",
       "  '3S_PERGUNTAR_3S ONDE MORAR [PONTO]'),\n",
       " ('ELE PERGUNTAR ELA ONDE VIVER [PONTO]',\n",
       "  '3S_PERGUNTAR_3S ONDE VIVER [PONTO]'),\n",
       " ('1991 DALE BROWN CONVOCAR 2 CONSAGRAR EX_JOGADOR NBA PARA AUXILIAR CRESCIMENTO SHAQUILLE KAREEM ABDUL-JABBAR BILL WALTON [PONTO] KAREEM ENSINAR SHAQ SEU FAMOSO SKY HOOK ENQUANTO WALTON ENSINAR MOVIMENTO OFENSIVO TÉCNICA BLOQUEIO [PONTO] WALTON FICAR IMPRESSIONAR COM CAPACIDADE SHAQ ELE ME LEMBRAR CHARLES BARKLEY [PONTO] SHAQUILLE TER AQUELE RÁPIDA INCONTROLÁVEL EXPLOSÃO COMO BARKLEY [PONTO] SER FORÇA BRUTO QUE VOCÊ NÃO_CONSEGUIR SALA MUSCULAR [PONTO] VIR ALGUM OUTRO LUGAR FUNDO ALMA [PONTO] GAROTO DEVER TER TALENTO FÍSICO DISCIPLINA PESSOAL PARA SER MELHOR TODO [PONTO]',\n",
       "  'ANO_DATA 1991 DALE BROWN CONVOCAR 2 CONSAGRADO EX JOGADOR NBA AUXILIAR CRESCIMENTO SHAQUILLE KAREEM ABDUL JABBAR BILL WALTON [PONTO] KAREEM 1S_ENSINAR_2S SHAQ FAMOSO SKY HOOK ENQUANTO WALTON 1S_ENSINAR_2S MOVIMENTO OFENSIVO TÉCNICA BLOQUEIO [PONTO] WALTON IMPRESSIONADO CAPACIDADE SHAQ LEMBRAR CHARLES BARKLEY [PONTO] SHAQUILLE TER RÁPIDA INCONTROLÁVEL EXPLOSÃO BARKLEY FORÇA NÃO_CONSEGUIR SALA MUSCULAÇÃO [PONTO] VIR OUTRO LUGAR ALMA [PONTO] GAROTO DEVER_OBRIGAÇÃO TALENTO FÍSICO DISCIPLINA PESSOAL MELHOR'),\n",
       " ('ENQUANTO DENTRO HOSPITAL SARAH QUE ANTES TER ATACAR DR [PONTO] SILBERMANN QUE NEGAR DIREITO VER SEU FILHO FINGIR ESTAR DESLIGAR REALIDADE NEM MESMO QUANDO POLICIAL MOSTRAR FOTO EXTERMINADOR SHOPPING FOTO DELEGACIA MASSACRE 1984 DEMONSTRAR CONSCIÊNCIA [PONTO] MAS SER SÓ FINGIMENTO ESCONDER CLIPE SEU BOCA QUE FORA USAR PARA ABRIR_PORTA ONDE ESTAR PRESA INIR DIREÇÃO SILBERMANN QUEBRAR SEU BRAÇO CHANTAGEAR GUARDA HOSPITAL ABRIR_PORTA CASO ELES NEGAR ELA INJETAR VENENO COM SERINGA SILBERMANN [PONTO]',\n",
       "  'DENTRO_AÇÃO HOSPITAL SARAH ANTES_ANTERIOR ATACAR_AGREDIR DOUTOR [PONTO] SILBERMANN NEGAR DIREITO_LEI VER FILHO FINGIR  NÃO_SABER REALIDADE  IGUAL POLICIAL HOMEM 1S_MOSTRAR_2S FOTO EXTERMINADOR SHOPPING FOTO DELEGACIA MASSACRE ANO_DATA 1984 DEMONSTRAR CONSCIÊNCIA [PONTO] FINGIMENTO ESCONDER CLIPE BOCA ABRIR_PORTA ONDE PRESA DIREÇÃO SILBERMANN QUEBRAR_DESTRUIR BRAÇO CHANTAGEAR GUARDAR_VIGIAR HOSPITAL ABRIR_PORTA SE ELES NEGAR ELA INJETAR VENENO SERINGA SILBERMANN [PONTO]'),\n",
       " ('DESCULPA SE EU TE ASSUSTAR [PONTO]', 'DESCULPAR SE 1S_ASSUSTAR_2S [PONTO]'),\n",
       " ('30 MARÇO 1967 IR REALIZAR SESSÃO FOTO COM COOPER QUE TAMBÉM PRODUZIR IMAGEM CONTRACAPA ENCARTAR QUAL MUSICÓLOGO IAN INGLIS ACREDITAR TRANSMITIR ÓBVIO IMEDIATO CALOR QUE DISTANCIAR ESTERILIDADE ARTIFICIALIDADE TÍPICO TAL IMAGEM PRODUZIR ESTÚDIO FOTOGRAFIA [PONTO] MCCARTNEY EXPLICAR COISA QUE MAIS NOS INTERESSAR AQUELES TEMPO SER MENSAGEM VISUAL ENTÃO SESSÃO PARA FOTO INTERNO MICHAEL COOPER NOS FALAR AGORA OLHAR PARA ESSE CÂMERA FALAR REAL EU TE AMOR [EXCLAMAÇÃO] FATO TENTAR SENTIR AMOR DAR AMOR ISSO [EXCLAMAÇÃO] IR DAR CERTO IR TRANSPARECER SER ATITUDE [PONTO] IR QUE ACONTECER SE VOCÊ OLHAR PARA ELA FOTO VOCÊ IR VER NOSSOS OLHO GRANDE ESFORÇO [PONTO] LUVA INTERNO ÁLBUM APRESENTAR ARTE COLETIVO NEERLANDÊS DESIGN GRÁFICO THE FOOL QUE EVITAR PRIMEIRA_VEZ USO PAPEL BRANCO COMUM FAVOR TIPO MATERIAL COM PADRÃO ABSTRATO ONDA CASTANHO VERMELHO ROSA BRANCO [PONTO]',\n",
       "  'DIA 30 MÊS MARÇO ANO_DATA 1967 REALIZADA SESSÃO FOTO COOPER TAMBÉM PRODUZIR IMAGEM CONTRA_CAPA ENCARTE MUSICÓLOGO IAN INGLIS ACREDITAR TRANSMITIR ÓBVIO IMEDIATO CALOR DISTANCIAR ESTERILIDADE ARTIFICIALIDADE TÍPICA IMAGEM PRODUZIR ESTÚDIO FOTOGRAFIA [PONTO] MCCARTNEY EXPLICAR MAIS_QUANTIDADE  INTERESSAR NAQUELE TEMPO_ÉPOCA MENSAGEM VISUAL SESSÃO FOTO INTERNA MICHAEL COOPER DIZER AGORA OLHAR_VER CÂMERA DIZER REAL EU AMAR [EXCLAMAÇÃO] SENTIR AMOR 1S_DAR_2S AMOR NISSO [EXCLAMAÇÃO] TRANSPARECER ATITUDE [PONTO] ACONTECER VOCÊ OLHAR_VER ELA FOTO VOCÊ VER NOSSO OLHO GRANDE ESFORÇO [PONTO] [PONTO] LUVA INTERNA ÁLBUM_FOTO APRESENTAR_MOSTRAR ARTE COLETIVO NEERLANDÊS DESIGN GRÁFICO THE FOOL EVITAR PRIMEIRO_ORDINAL USAR PAPEL BRANCO NORMAL FAVOR TIPO MATERIAL PADRÃO ABSTRATO ONDA CASTANHO VERMELHO ROSA BRANCO [PONTO]'),\n",
       " ('ELE ME PERGUNTAR SOBRE MEU NOVO TRABALHO [PONTO]',\n",
       "  'ELE 3S_PERGUNTAR_1S  SOBRE_ASSUNTO MEU NOVO_RECENTE TRABALHO [PONTO]'),\n",
       " ('ELES ME ENSINAR JURAR [PONTO]', 'ELES 2S_ENSINAR_1S JURAR [PONTO]'),\n",
       " ('ELES ME DAR OUTRO AUMENTO [PONTO]', 'ELES 3S_DAR_1S OUTRO AUMENTO [PONTO]'),\n",
       " ('ELES ME DAR QUE EU QUERER [PONTO]', 'ELES 3S_DAR_1S QUE QUERER [PONTO]'),\n",
       " ('ESPERAR QUE ELE ME AJUDAR [PONTO]', 'ESPERAR 3S_AJUDAR_1S [PONTO]'),\n",
       " ('EU TE DAR ESSE LIVRO [PONTO]', 'EU 1S_DAR_2S LIVRO VOCÊ [PONTO]'),\n",
       " ('EU ME LIVRAR MEU ANTIGO BICICLETA [PONTO]',\n",
       "  'EU 1S_DAR_2S MINHA ANTIGA BICICLETA [PONTO]'),\n",
       " ('EU ME PERGUNTAR SE TOM NOS AJUDAR [PONTO]',\n",
       "  'EU 1S_PERGUNTAR_2S TOM AJUDAR [PONTO]'),\n",
       " ('SEU JUSTIFICAR IR EU ME MACHUCAR DURANTE MEU TEMPO SERVIÇO ENTÃO EU IR ME RECUPERAR DURANTE MEU TEMPO SERVIÇO [PONTO] LAKERS NÃO_APRESENTAR BOM RENDIMENTO PRIMEIROS 30 JOGO TEMPORADA FAZER 11 VÍRGULA 19 [PONTO] SHAQ TER MÉDIO 27 VÍRGULA 5 PONTO 57 VÍRGULA 4 APROVEITAMENTO 11 VÍRGULA 1 REBOTES 2 VÍRGULA 4 TOCOS 3 VÍRGULA 1 ASSISTÊNCIA DURANTE TEMPORADA REGULAR IR INCLUIR MELHOR QUINTETO NBA SEGUNDO MELHOR QUINTETO DEFENSIVO LIGA [PONTO]',\n",
       "  'JUSTIFICAR 2S_MACHUCAR_1S TEMPO_ÉPOCA SERVIÇO ENTÃO RECUPERAR TEMPO_ÉPOCA SERVIÇO MEU [PONTO] [PONTO] LAKERS NÃO_APRESENTAR BOM RENDIMENTO PRIMEIRO 30 JOGO TEMPORADA CONSEGUIR 11 ATÉ 19 [PONTO] SHAQ TER MÉDIA_NÍVEL 27 VÍRGULA 5 PONTO_MARCA 57 VÍRGULA 4 PORCENTAGEM APROVEITAR_UTILIZAR 11 VÍRGULA 1 REBOTE 2 VÍRGULA 4 TOCO 3 VÍRGULA 1 ASSISTÊNCIA TEMPORADA REGULAR INCLUIR MELHOR QUINTETO NBA SEGUNDO_SEQUÊNCIA MELHOR QUINTETO DEFENSIVO LIGA_GRUPO [PONTO]'),\n",
       " ('QUERER QUE EU TE AJUDAR [INTERROGAÇÃO]',\n",
       "  'QUER 1S_AJUDAR_2S [INTERROGAÇÃO]')]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('phrases_augmentation.txt','w') as file:\n",
    "    \n",
    "    for phrase in phrases_augmentation:\n",
    "        file.write(f'{phrase}\\n')\n",
    "phrases_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total de frases [agente -> pronome -> verbo] [agente -> verbo -> receptor]\n",
    "len(phrases_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EU ME PERGUNTAR PORQUÊ ÔNIBUS ESTAR ATRASADO [PONTO]',\n",
       " '1S_PERGUNTAR_2S PORQUE ÔNIBUS ATRASAR [PONTO]')"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frases [agente -> pronome -> verbo] [agente -> verbo -> receptor]\n",
    "phrases_augmentation[random.randint(0,len(phrases_augmentation))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste com frases para augmentation encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2592"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = Directionality_Augmentation()\n",
    "new_phrases = da.process(phrases_augmentation,max_new_sentences = 555555)\n",
    "len(new_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
